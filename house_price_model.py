# -*- coding: utf-8 -*-
"""House Price model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B48P2fJxoUV8_zFM0F5YtNuDhP42-gMa
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Load your dataset
# Replace 'your_dataset.csv' with the actual file name
dataset = pd.read_excel('Final Data (2).xlsx')
dataset.head()

# Get unique GEO_Names from the dataset
geo_names = dataset['GEO_Name'].unique()

# Loop through each GEO_Name
for geo_name in geo_names:
    # Filter data for the specific GEO_Name
    geo_data = dataset[dataset['GEO_Name'] == geo_name]

    # Check if there are samples for the current GEO_Name
    if geo_data.shape[0] == 0:
        print(f"No data found for {geo_name}. Skipping...")
        continue

    # Split data into training and validation sets
    training_data_geo = geo_data[geo_data['Year'] <= 2019]
    validation_future_data_geo = geo_data[geo_data['Year'] > 2019]

    # Extract features and target variable
    features = ['CPI', 'Median Income', 'MORTGAGE', 'Population', 'Unemployment Rate', 'GDP']
    target_variable = 'Index_NSA'

    # Preprocess data
    scaler = MinMaxScaler()
    X_train_geo = scaler.fit_transform(training_data_geo[features])
    y_train_geo = training_data_geo[target_variable].values

    X_val_future_geo = scaler.transform(validation_future_data_geo[features])
    y_val_future_geo = validation_future_data_geo[target_variable].values

    # Set up the RNN model with increased complexity
    model_geo = Sequential()
    model_geo.add(LSTM(units=200, activation='relu', input_shape=(X_train_geo.shape[1], 1), return_sequences=True))
    model_geo.add(LSTM(units=100, activation='relu'))
    model_geo.add(Dense(units=1))

    # Compile the model with the specified learning rate
    model_geo.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')

    # Train the model with increased patience for early stopping
    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
    model_geo.fit(X_train_geo, y_train_geo, epochs=50, batch_size=32, validation_data=(X_val_future_geo, y_val_future_geo), callbacks=[early_stopping], verbose=2)

    # Make predictions for the future values
    X_future_geo = scaler.transform(validation_future_data_geo[features])
    predictions_future_geo = model_geo.predict(X_future_geo)

    # Print or use the predictions_future_geo as needed
    print(f"Predictions for {geo_name}:")
    print(predictions_future_geo)

# Handle missing values
training_data = geo_data[geo_data['Year'] <= 2019]
validation_future_data = geo_data[geo_data['Year'] > 2019]

training_data = training_data.dropna()
validation_future_data_geo = validation_future_data.dropna()

# Handle infinite values
training_data = training_data.replace([np.inf, -np.inf], np.nan).dropna()
validation_future_data = validation_future_data.replace([np.inf, -np.inf], np.nan).dropna()

# Define function for preparing data for RNN
def prepare_rnn_data(data, sequence_length):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data[features].iloc[i:i+sequence_length].values)
        y.append(data[target_variable].iloc[i+sequence_length])
    return np.array(X), np.array(y)

# Set sequence length
sequence_length = 10  # You may need to adjust this based on your data

# Prepare data for RNN
X_train, y_train = prepare_rnn_data(training_data, sequence_length)
X_val_future, y_val_future = prepare_rnn_data(validation_future_data, sequence_length)

# Create and train the RNN model
def create_rnn_model(input_shape):
    model = Sequential()
    model.add(LSTM(units=100, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dense(units=1))
    model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')
    return model

model = create_rnn_model((X_train.shape[1], X_train.shape[2]))
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val_future, y_val_future))

# Make predictions for future values
X_future = prepare_rnn_data(validation_future_data, sequence_length)[0]
predictions_future = model.predict(X_future)

# Optionally, you can inverse transform the predictions if you scaled the data before training
# predictions_future = scaler.inverse_transform(predictions_future)

# Print or use the predictions_future as needed
print("Predictions for the future:")
print(predictions_future)

from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt

# Evaluate the model on the training set
train_predictions = model.predict(X_train)
train_rmse = sqrt(mean_squared_error(y_train, train_predictions))
train_mse = mean_squared_error(y_train, train_predictions)
train_mae = mean_absolute_error(y_train, train_predictions)

print(f"Training RMSE: {train_rmse}")
print(f"Training MSE: {train_mse}")
print(f"Training MAE: {train_mae}")

# Evaluate the model on the validation set
val_predictions_future = model.predict(X_val_future)
val_rmse_future = sqrt(mean_squared_error(y_val_future, val_predictions_future))
val_mse_future = mean_squared_error(y_val_future, val_predictions_future)
val_mae_future = mean_absolute_error(y_val_future, val_predictions_future)

print(f"Validation Future RMSE: {val_rmse_future}")
print(f"Validation Future MSE: {val_mse_future}")
print(f"Validation Future MAE: {val_mae_future}")

# Evaluate the model on the entire validation set (including past and future data)
X_val = prepare_rnn_data(validation_future_data, sequence_length)[0]
y_val = prepare_rnn_data(validation_future_data, sequence_length)[1]
val_predictions = model.predict(X_val)
val_rmse = sqrt(mean_squared_error(y_val, val_predictions))
val_mse = mean_squared_error(y_val, val_predictions)
val_mae = mean_absolute_error(y_val, val_predictions)

print(f"Validation RMSE (Entire): {val_rmse}")
print(f"Validation MSE (Entire): {val_mse}")
print(f"Validation MAE (Entire): {val_mae}")

threshold = 0.1  # For example, consider predictions within 10% of the true value as correct

# Calculate accuracy for training set
train_accuracy = np.mean(np.abs(train_predictions - y_train) / y_train <= threshold)
print(f"Training Accuracy: {train_accuracy}")

# Calculate accuracy for validation set (future data)
val_accuracy_future = np.mean(np.abs(val_predictions_future - y_val_future) / y_val_future <= threshold)
print(f"Validation Future Accuracy: {val_accuracy_future}")

# Calculate accuracy for entire validation set (including past and future data)
val_accuracy = np.mean(np.abs(val_predictions - y_val) / y_val <= threshold)
print(f"Validation Accuracy (Entire): {val_accuracy}")

import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt

# Load your dataset
dataset = pd.read_excel('Final Data .xlsx')

# Create DateTime index
dataset['datetime'] = pd.to_datetime(dataset[['Year', 'Month']].assign(DAY=1))
dataset.set_index('datetime', inplace=True)

# Select features and target
features = ['CPI', 'Median Income', 'MORTGAGE', 'Population', 'Unemployment Rate', 'GDP']
target_variable = 'Index_SA'

# Filter columns
selected_columns = features + [target_variable]

# Filter and preprocess data
model_data = dataset[selected_columns].dropna()

# Fit SARIMA model
order = (1, 1, 1)  # Example order, you may need to tune these parameters
seasonal_order = (1, 1, 1, 12)  # Example seasonal order, adjust as needed

# Split the data into training and validation sets
train_data = model_data.loc[model_data.index <= '2015-12-31']
validation_data = model_data.loc[(model_data.index > '2015-12-31') & (model_data.index <= '2019-01-31')]

sarima_model = SARIMAX(train_data[target_variable], order=order, seasonal_order=seasonal_order)
sarima_result = sarima_model.fit()


# Make predictions
start_date = validation_data.index[0]
end_date = validation_data.index[-1]
sarima_predictions = sarima_result.predict(start=len(train_data), end=len(train_data) + len(validation_data) - 1, dynamic=True)

# Evaluate SARIMA model
rmse_sarima = sqrt(mean_squared_error(validation_data[target_variable], sarima_predictions))
mse_sarima = mean_squared_error(validation_data[target_variable], sarima_predictions)
mae_sarima = mean_absolute_error(validation_data[target_variable], sarima_predictions)

# Print evaluation metrics
print(f"SARIMA RMSE: {rmse_sarima}")
print(f"SARIMA MSE: {mse_sarima}")
print(f"SARIMA MAE: {mae_sarima}")

pip install prophet

import pandas as pd
from prophet import Prophet
from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt

# Assuming your dataset is stored in a DataFrame named 'df'
# Make sure to install the required libraries: pip install pandas fbprophet scikit-learn

# Load your dataset
df = pd.read_excel('Final Data (2).xlsx')

# Combine 'Year' and 'Month' columns into a single datetime column
df['ds'] = pd.to_datetime(df[['Year', 'Month']].assign(DAY=1))

# Fill NaN values in the 'population' column with the mean
df['Population'].fillna(df['Population'].mean(), inplace=True)

# Prepare the data for Prophet
prophet_data = df[['ds', 'CPI', 'Median Income', 'MORTGAGE', 'Population', 'Unemployment Rate', 'GDP', 'GEO_Name', 'Index_NSA']].rename(columns={'ds': 'ds', 'CPI': 'cpi', 'Median Income': 'median_income', 'MORTGAGE': 'mortgage', 'Population': 'population', 'Unemployment Rate': 'unemployment_rate', 'GDP': 'gdp', 'GEO_Name': 'region', 'Index_NSA': 'y'})

# Create a separate model for each GEO_Name
geo_names = df['GEO_Name'].unique()
models = {}
mae_values = []
mse_values = []
rmse_values = []

for geo_name in geo_names:
    # Filter data for the current GEO_Name
    geo_data = prophet_data[prophet_data['region'] == geo_name]

    # Check if 'cpi' regressor is present in the region's data
    if 'cpi' not in geo_data.columns:
        geo_data['cpi'] = 0  # You can replace 0 with a suitable default value or use a different imputation method

    # Split data into training and validation
    train_data = geo_data[geo_data['ds'] < '2020-01-01']
    valid_data = geo_data[geo_data['ds'] >= '2020-01-01']

    # Create and fit the Prophet model
    model = Prophet()
    model.add_regressor('cpi')
    model.add_regressor('median_income')
    model.add_regressor('mortgage')
    model.add_regressor('population')
    model.add_regressor('unemployment_rate')
    model.add_regressor('gdp')

    model.fit(train_data)

    # Make predictions on the validation set
    forecast = model.predict(valid_data)

    # Store the model for later use
    models[geo_name] = model

    # Calculate and append accuracy metrics
    mae = mean_absolute_error(valid_data['y'], forecast['yhat'])
    mse = mean_squared_error(valid_data['y'], forecast['yhat'])
    rmse = sqrt(mse)

    mae_values.append(mae)
    mse_values.append(mse)
    rmse_values.append(rmse)

    # Plot the predictions
    fig = model.plot(forecast)
    fig.suptitle(f'Prophet Forecast for {geo_name}')

# Calculate combined metrics
combined_mae = sum(mae_values) / len(mae_values)
combined_mse = sum(mse_values) / len(mse_values)
combined_rmse = sum(rmse_values) / len(rmse_values)

print(f"Combined MAE: {combined_mae}")
print(f"Combined MSE: {combined_mse}")
print(f"Combined RMSE: {combined_rmse}")

# To make future predictions:
future = model.make_future_dataframe(periods=12, freq='M')